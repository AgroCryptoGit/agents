<html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><link rel="preload" as="image" href="/flows/01 sequential.png"/><link rel="preload" as="image" href="/flows/02 routing.png"/><link rel="preload" as="image" href="/flows/03 parallel.png"/><link rel="preload" as="image" href="/flows/04 orchestrator.png"/><link rel="preload" as="image" href="/flows/05 evaluator.png"/><title>Anthropic Patterns</title><link rel="stylesheet" href="/normalize.css"/><link rel="stylesheet" href="/dist/client.css"/></head><body><div id="app"><div class="container"><div class="toast-container"></div><header><div class="theme-toggle"><span class="theme-toggle-icon">üåû</span><div class="theme-toggle-switch"></div></div><h1>‚õÖÔ∏è Building Effective Agents</h1><p>Common patterns for implementing AI agents</p><div class="header-links"><p>Based on<!-- --> <a href="https://www.anthropic.com/research/building-effective-agents" target="_blank" rel="noopener noreferrer">Anthropic&#x27;s research</a> <!-- -->on agent patterns.</p><p>Code samples from<!-- --> <a href="https://sdk.vercel.ai/docs/foundations/agents" target="_blank" rel="noopener noreferrer">AI SDK</a>, running in Cloudflare&#x27;s Durable Objects.</p></div></header><main><section class="pattern-section"><h2>1<!-- -->. <!-- -->Prompt Chaining</h2><div class="pattern-content"><div class="tab-container"><div class="tab-buttons"><button class="tab-button active">Diagram</button><button class="tab-button ">Code</button></div><div class="tab-content"><div class="tab-pane active"><div class="pattern-image"><img src="/flows/01 sequential.png" alt="Prompt Chaining workflow diagram"/></div></div><div class="tab-pane "><div class="code-tab-container"><div class="code-content ">import { openai } from &quot;@ai-sdk/openai&quot;;
import { generateText, generateObject } from &quot;ai&quot;;
import { z } from &quot;zod&quot;;

export default async function generateMarketingCopy(input: string) {
  const model = openai(&quot;gpt-4o&quot;);

  // First step: Generate marketing copy
  const { text: copy } = await generateText({
    model,
    prompt: `Write persuasive marketing copy for: ${input}. Focus on benefits and emotional appeal.`,
  });

  // Perform quality check on copy
  const { object: qualityMetrics } = await generateObject({
    model,
    schema: z.object({
      hasCallToAction: z.boolean(),
      emotionalAppeal: z.number().min(1).max(10),
      clarity: z.number().min(1).max(10),
    }),
    prompt: `Evaluate this marketing copy for:
    1. Presence of call to action (true/false)
    2. Emotional appeal (1-10)
    3. Clarity (1-10)

    Copy to evaluate: ${copy}`,
  });

  // If quality check fails, regenerate with more specific instructions
  if (
    !qualityMetrics.hasCallToAction ||
    qualityMetrics.emotionalAppeal &lt; 7 ||
    qualityMetrics.clarity &lt; 7
  ) {
    const { text: improvedCopy } = await generateText({
      model,
      prompt: `Rewrite this marketing copy with:
      ${!qualityMetrics.hasCallToAction ? &quot;- A clear call to action&quot; : &quot;&quot;}
      ${qualityMetrics.emotionalAppeal &lt; 7 ? &quot;- Stronger emotional appeal&quot; : &quot;&quot;}
      ${qualityMetrics.clarity &lt; 7 ? &quot;- Improved clarity and directness&quot; : &quot;&quot;}

      Original copy: ${copy}`,
    });
    return { copy: improvedCopy, qualityMetrics };
  }

  return { copy, qualityMetrics };
}
</div><button class="expand-button">Expand<span class="icon ">‚ñº</span></button></div></div></div></div><p class="pattern-description">Decomposes tasks into a sequence of steps, where each LLM call processes the output of the previous one.</p><div class="workflow-runner"><div class="workflow-form"><div class="form-group"><label for="sequential-input">Marketing Copy Input</label><input id="sequential-input" type="text" placeholder="e.g., &#x27;Our new AI-powered productivity app&#x27;" class="workflow-input" name="input" value="Our new AI-powered productivity app"/><small class="input-help">Enter a product or service to generate marketing copy for</small></div></div><div class="workflow-toolbar"><button class="run-button">Run</button></div><pre class="workflow-output">Enter input above and click &#x27;Run&#x27; to see Prompt Chaining in action</pre></div></div></section><section class="pattern-section"><h2>2<!-- -->. <!-- -->Routing</h2><div class="pattern-content"><div class="tab-container"><div class="tab-buttons"><button class="tab-button active">Diagram</button><button class="tab-button ">Code</button></div><div class="tab-content"><div class="tab-pane active"><div class="pattern-image"><img src="/flows/02 routing.png" alt="Routing workflow diagram"/></div></div><div class="tab-pane "><div class="code-tab-container"><div class="code-content ">import { openai } from &#x27;@ai-sdk/openai&#x27;;
import { generateObject, generateText } from &#x27;ai&#x27;;
import { z } from &#x27;zod&#x27;;

async function handleCustomerQuery(query: string) {
  const model = openai(&#x27;gpt-4o&#x27;);

  // First step: Classify the query type
  const { object: classification } = await generateObject({
    model,
    schema: z.object({
      reasoning: z.string(),
      type: z.enum([&#x27;general&#x27;, &#x27;refund&#x27;, &#x27;technical&#x27;]),
      complexity: z.enum([&#x27;simple&#x27;, &#x27;complex&#x27;]),
    }),
    prompt: `Classify this customer query:
    ${query}

    Determine:
    1. Query type (general, refund, or technical)
    2. Complexity (simple or complex)
    3. Brief reasoning for classification`,
  });

  // Route based on classification
  // Set model and system prompt based on query type and complexity
  const { text: response } = await generateText({
    model:
      classification.complexity === &#x27;simple&#x27;
        ? openai(&#x27;gpt-4o-mini&#x27;)
        : openai(&#x27;o1-mini&#x27;),
    system: {
      general:
        &#x27;You are an expert customer service agent handling general inquiries.&#x27;,
      refund:
        &#x27;You are a customer service agent specializing in refund requests. Follow company policy and collect necessary information.&#x27;,
      technical:
        &#x27;You are a technical support specialist with deep product knowledge. Focus on clear step-by-step troubleshooting.&#x27;,
    }[classification.type],
    prompt: query,
  });

  return { response, classification };
}</div><button class="expand-button">Expand<span class="icon ">‚ñº</span></button></div></div></div></div><p class="pattern-description">Classifies input and directs it to specialized followup tasks, allowing for separation of concerns.</p><div class="workflow-runner"><div class="workflow-form"><div class="form-group"><label for="routing-query">Customer Query</label><input id="routing-query" type="text" placeholder="e.g., &#x27;How do I reset my password?&#x27;" class="workflow-input" name="query" value="How do I reset my password?"/><small class="input-help">Enter a customer support question to be routed</small></div></div><div class="workflow-toolbar"><button class="run-button">Run</button></div><pre class="workflow-output">Enter input above and click &#x27;Run&#x27; to see Routing in action</pre></div></div></section><section class="pattern-section"><h2>3<!-- -->. <!-- -->Parallelization</h2><div class="pattern-content"><div class="tab-container"><div class="tab-buttons"><button class="tab-button active">Diagram</button><button class="tab-button ">Code</button></div><div class="tab-content"><div class="tab-pane active"><div class="pattern-image"><img src="/flows/03 parallel.png" alt="Parallelization workflow diagram"/></div></div><div class="tab-pane "><div class="code-tab-container"><div class="code-content ">import { openai } from &#x27;@ai-sdk/openai&#x27;;
import { generateText, generateObject } from &#x27;ai&#x27;;
import { z } from &#x27;zod&#x27;;

// Example: Parallel code review with multiple specialized reviewers
async function parallelCodeReview(code: string) {
  const model = openai(&#x27;gpt-4o&#x27;);

  // Run parallel reviews
  const [securityReview, performanceReview, maintainabilityReview] =
    await Promise.all([
      generateObject({
        model,
        system:
          &#x27;You are an expert in code security. Focus on identifying security vulnerabilities, injection risks, and authentication issues.&#x27;,
        schema: z.object({
          vulnerabilities: z.array(z.string()),
          riskLevel: z.enum([&#x27;low&#x27;, &#x27;medium&#x27;, &#x27;high&#x27;]),
          suggestions: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),

      generateObject({
        model,
        system:
          &#x27;You are an expert in code performance. Focus on identifying performance bottlenecks, memory leaks, and optimization opportunities.&#x27;,
        schema: z.object({
          issues: z.array(z.string()),
          impact: z.enum([&#x27;low&#x27;, &#x27;medium&#x27;, &#x27;high&#x27;]),
          optimizations: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),

      generateObject({
        model,
        system:
          &#x27;You are an expert in code quality. Focus on code structure, readability, and adherence to best practices.&#x27;,
        schema: z.object({
          concerns: z.array(z.string()),
          qualityScore: z.number().min(1).max(10),
          recommendations: z.array(z.string()),
        }),
        prompt: `Review this code:
      ${code}`,
      }),
    ]);

  const reviews = [
    { ...securityReview.object, type: &#x27;security&#x27; },
    { ...performanceReview.object, type: &#x27;performance&#x27; },
    { ...maintainabilityReview.object, type: &#x27;maintainability&#x27; },
  ];

  // Aggregate results using another model instance
  const { text: summary } = await generateText({
    model,
    system: &#x27;You are a technical lead summarizing multiple code reviews.&#x27;,
    prompt: `Synthesize these code review results into a concise summary with key actions:
    ${JSON.stringify(reviews, null, 2)}`,
  });

  return { reviews, summary };
}</div><button class="expand-button">Expand<span class="icon ">‚ñº</span></button></div></div></div></div><p class="pattern-description">Enables simultaneous task processing through sectioning or voting mechanisms.</p><div class="workflow-runner"><div class="workflow-form"><div class="form-group"><label for="parallel-code">Code for Review</label><textarea id="parallel-code" name="code" placeholder="e.g.,
function processUserData(data) {
  // TODO: Add validation
  database.save(data);
  return true;
}" class="workflow-input workflow-textarea" rows="4">function processUserData(data) {
  // TODO: Add validation
  database.save(data);
  return true;
}</textarea><small class="input-help">Enter code snippet for parallel security, performance, and maintainability review</small></div></div><div class="workflow-toolbar"><button class="run-button">Run</button></div><pre class="workflow-output">Enter input above and click &#x27;Run&#x27; to see Parallelization in action</pre></div></div></section><section class="pattern-section"><h2>4<!-- -->. <!-- -->Orchestrator-Workers</h2><div class="pattern-content"><div class="tab-container"><div class="tab-buttons"><button class="tab-button active">Diagram</button><button class="tab-button ">Code</button></div><div class="tab-content"><div class="tab-pane active"><div class="pattern-image"><img src="/flows/04 orchestrator.png" alt="Orchestrator-Workers workflow diagram"/></div></div><div class="tab-pane "><div class="code-tab-container"><div class="code-content ">import { openai } from &#x27;@ai-sdk/openai&#x27;;
import { generateObject } from &#x27;ai&#x27;;
import { z } from &#x27;zod&#x27;;

async function implementFeature(featureRequest: string) {
  // Orchestrator: Plan the implementation
  const { object: implementationPlan } = await generateObject({
    model: openai(&#x27;o1&#x27;),
    schema: z.object({
      files: z.array(
        z.object({
          purpose: z.string(),
          filePath: z.string(),
          changeType: z.enum([&#x27;create&#x27;, &#x27;modify&#x27;, &#x27;delete&#x27;]),
        }),
      ),
      estimatedComplexity: z.enum([&#x27;low&#x27;, &#x27;medium&#x27;, &#x27;high&#x27;]),
    }),
    system:
      &#x27;You are a senior software architect planning feature implementations.&#x27;,
    prompt: `Analyze this feature request and create an implementation plan:
    ${featureRequest}`,
  });

  // Workers: Execute the planned changes
  const fileChanges = await Promise.all(
    implementationPlan.files.map(async file =&gt; {
      // Each worker is specialized for the type of change
      const workerSystemPrompt = {
        create:
          &#x27;You are an expert at implementing new files following best practices and project patterns.&#x27;,
        modify:
          &#x27;You are an expert at modifying existing code while maintaining consistency and avoiding regressions.&#x27;,
        delete:
          &#x27;You are an expert at safely removing code while ensuring no breaking changes.&#x27;,
      }[file.changeType];

      const { object: change } = await generateObject({
        model: openai(&#x27;gpt-4o&#x27;),
        schema: z.object({
          explanation: z.string(),
          code: z.string(),
        }),
        system: workerSystemPrompt,
        prompt: `Implement the changes for ${file.filePath} to support:
        ${file.purpose}

        Consider the overall feature context:
        ${featureRequest}`,
      });

      return {
        file,
        implementation: change,
      };
    }),
  );

  return {
    plan: implementationPlan,
    changes: fileChanges,
  };
}</div><button class="expand-button">Expand<span class="icon ">‚ñº</span></button></div></div></div></div><p class="pattern-description">A central LLM dynamically breaks down tasks, delegates to worker LLMs, and synthesizes results.</p><div class="workflow-runner"><div class="workflow-form"><div class="form-group"><label for="orchestrator-request">Feature Request</label><textarea id="orchestrator-request" name="featureRequest" placeholder="e.g., &#x27;Add dark mode support to the dashboard, including theme persistence and system preference detection&#x27;" class="workflow-input workflow-textarea" rows="4">Add dark mode support to the dashboard, including theme persistence and system preference detection</textarea><small class="input-help">Describe the feature to be implemented across multiple files</small></div></div><div class="workflow-toolbar"><button class="run-button">Run</button></div><pre class="workflow-output">Enter input above and click &#x27;Run&#x27; to see Orchestrator-Workers in action</pre></div></div></section><section class="pattern-section"><h2>5<!-- -->. <!-- -->Evaluator-Optimizer</h2><div class="pattern-content"><div class="tab-container"><div class="tab-buttons"><button class="tab-button active">Diagram</button><button class="tab-button ">Code</button></div><div class="tab-content"><div class="tab-pane active"><div class="pattern-image"><img src="/flows/05 evaluator.png" alt="Evaluator-Optimizer workflow diagram"/></div></div><div class="tab-pane "><div class="code-tab-container"><div class="code-content ">import { openai } from &#x27;@ai-sdk/openai&#x27;;
import { generateText, generateObject } from &#x27;ai&#x27;;
import { z } from &#x27;zod&#x27;;

async function translateWithFeedback(text: string, targetLanguage: string) {
  let currentTranslation = &#x27;&#x27;;
  let iterations = 0;
  const MAX_ITERATIONS = 3;

  // Initial translation
  const { text: translation } = await generateText({
    model: openai(&#x27;gpt-4o-mini&#x27;), // use small model for first attempt
    system: &#x27;You are an expert literary translator.&#x27;,
    prompt: `Translate this text to ${targetLanguage}, preserving tone and cultural nuances:
    ${text}`,
  });

  currentTranslation = translation;

  // Evaluation-optimization loop
  while (iterations &lt; MAX_ITERATIONS) {
    // Evaluate current translation
    const { object: evaluation } = await generateObject({
      model: openai(&#x27;gpt-4o&#x27;), // use a larger model to evaluate
      schema: z.object({
        qualityScore: z.number().min(1).max(10),
        preservesTone: z.boolean(),
        preservesNuance: z.boolean(),
        culturallyAccurate: z.boolean(),
        specificIssues: z.array(z.string()),
        improvementSuggestions: z.array(z.string()),
      }),
      system: &#x27;You are an expert in evaluating literary translations.&#x27;,
      prompt: `Evaluate this translation:

      Original: ${text}
      Translation: ${currentTranslation}

      Consider:
      1. Overall quality
      2. Preservation of tone
      3. Preservation of nuance
      4. Cultural accuracy`,
    });

    // Check if quality meets threshold
    if (
      evaluation.qualityScore &gt;= 8 &amp;&amp;
      evaluation.preservesTone &amp;&amp;
      evaluation.preservesNuance &amp;&amp;
      evaluation.culturallyAccurate
    ) {
      break;
    }

    // Generate improved translation based on feedback
    const { text: improvedTranslation } = await generateText({
      model: openai(&#x27;gpt-4o&#x27;), // use a larger model
      system: &#x27;You are an expert literary translator.&#x27;,
      prompt: `Improve this translation based on the following feedback:
      ${evaluation.specificIssues.join(&#x27;\n&#x27;)}
      ${evaluation.improvementSuggestions.join(&#x27;\n&#x27;)}

      Original: ${text}
      Current Translation: ${currentTranslation}`,
    });

    currentTranslation = improvedTranslation;
    iterations++;
  }

  return {
    finalTranslation: currentTranslation,
    iterationsRequired: iterations,
  };
}</div><button class="expand-button">Expand<span class="icon ">‚ñº</span></button></div></div></div></div><p class="pattern-description">One LLM generates responses while another provides evaluation and feedback in a loop.</p><div class="workflow-runner"><div class="workflow-form"><div class="form-group"><label for="evaluator-text">Text to Translate</label><textarea id="evaluator-text" name="text" placeholder="e.g., &#x27;The early bird catches the worm&#x27;" class="workflow-input workflow-textarea" rows="4">The early bird catches the worm</textarea><small class="input-help">Enter text to be translated and optimized</small></div><div class="form-group"><label for="evaluator-language">Target Language</label><select id="evaluator-language" name="targetLanguage" class="workflow-input"><option value="french" selected="">French</option><option value="spanish">Spanish</option><option value="japanese">Japanese</option><option value="german">German</option><option value="mandarin">Mandarin Chinese</option><option value="arabic">Arabic</option><option value="russian">Russian</option><option value="italian">Italian</option><option value="klingon">Klingon</option><option value="portuguese">Portuguese</option></select><small class="input-help">Select the language to translate into</small></div></div><div class="workflow-toolbar"><button class="run-button">Run</button></div><pre class="workflow-output">Enter input above and click &#x27;Run&#x27; to see Evaluator-Optimizer in action</pre></div></div></section></main><section class="durable-objects-intro"><h2>Why Durable Objects?</h2><p>Cloudflare&#x27;s Durable Objects provide the perfect environment for hosting AI agents:</p><div class="benefits-grid"><div class="benefit-card"><h3>Persistent State</h3><p>Agents continue running even when browser tabs are closed or refreshed, maintaining their state and context throughout long-running tasks.</p></div><div class="benefit-card"><h3>Real-time Updates</h3><p>WebSocket connections enable live streaming of agent progress, thoughts, and results directly to any connected client, providing immediate feedback.</p></div><div class="benefit-card"><h3>Global Scale</h3><p>Agents run at the edge, automatically scaling across Cloudflare&#x27;s worldwide network, ensuring low-latency responses regardless of user location.</p></div><div class="benefit-card"><h3>Flexible Triggers</h3><p>Agents can be activated through various means: HTTP requests, scheduled cron jobs, email handlers, or other server-side events.</p></div><div class="benefit-card"><h3>Memory Isolation</h3><p>Each agent runs in its own isolated environment, preventing resource contention and ensuring reliable performance.</p></div><div class="benefit-card"><h3>Cost Effective</h3><p>Pay only for the compute time your agents use, with no idle costs and automatic scaling based on demand.</p></div></div></section></div></div><script type="module" src="/dist/client.js"></script></body></html>